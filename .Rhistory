# D-values higher than 4/(n-k-1)
cutoff <- 4/((nrow(mlrdata)-length(m2.mlr$coefficients)-2))
# start plotting
par(mfrow = c(1, 2))           # display plots in 1 row/2 columns
qqPlot(m2.mlr, main="QQ Plot") # create qq-plot
plot(m2.mlr, which=4, cook.levels = cutoff); par(mfrow = c(1, 1))
# add model diagnostics to the data
mlrdata <- mlrdata %>%
dplyr::mutate(residuals = resid(m2.mlr),
standardized.residuals = rstandard(m2.mlr),
studentized.residuals = rstudent(m2.mlr),
cooks.distance = cooks.distance(m2.mlr),
dffit = dffits(m2.mlr),
leverage = hatvalues(m2.mlr),
covariance.ratios = covratio(m2.mlr),
fitted = m2.mlr$fitted.values)
# plot 5
p5 <- ggplot(mlrdata,
aes(studentized.residuals)) +
theme(legend.position = "none")+
geom_histogram(aes(y=..density..),
binwidth = .2,
colour="black",
fill="gray90") +
labs(x = "Studentized Residual", y = "Density") +
stat_function(fun = dnorm,
args = list(mean = mean(mlrdata$studentized.residuals, na.rm = TRUE),
sd = sd(mlrdata$studentized.residuals, na.rm = TRUE)),
colour = "red", size = 1) +
theme_bw(base_size = 8)
# plot 6
p6 <- ggplot(mlrdata, aes(fitted, studentized.residuals)) +
geom_point() +
geom_smooth(method = "lm", colour = "Red")+
theme_bw(base_size = 8)+
labs(x = "Fitted Values",
y = "Studentized Residual")
# plot 7
p7 <- qplot(sample = mlrdata$studentized.residuals, stat="qq") +
theme_bw(base_size = 8) +
labs(x = "Theoretical Values",
y = "Observed Values")
?qplot()
# plot 7
p7 <- plot(sample = mlrdata$studentized.residuals, stat="qq") +
theme_bw(base_size = 8) +
labs(x = "Theoretical Values",
y = "Observed Values")
# plot 7
p7 <- quickplot(sample = mlrdata$studentized.residuals, stat="qq") +
theme_bw(base_size = 8) +
labs(x = "Theoretical Values",
y = "Observed Values")
# plot 7
p7 <- ggplot(sample = mlrdata$studentized.residuals, stat="qq") +
theme_bw(base_size = 8) +
labs(x = "Theoretical Values",
y = "Observed Values")
vip::grid.arrange(p5, p6, p7, nrow = 1)
# 1: optimal = 0
# (listed data points should be removed)
which(mlrdata$standardized.residuals > 3.29)
# 2: optimal = 1
# (listed data points should be removed)
stdres_258 <- as.vector(sapply(mlrdata$standardized.residuals, function(x) {
ifelse(sqrt((x^2)) > 2.58, 1, 0) } ))
(sum(stdres_258) / length(stdres_258)) * 100
# 3: optimal = 5
# (listed data points should be removed)
stdres_196 <- as.vector(sapply(mlrdata$standardized.residuals, function(x) {
ifelse(sqrt((x^2)) > 1.96, 1, 0) } ))
(sum(stdres_196) / length(stdres_196)) * 100
# 4: optimal = 0
# (listed data points should be removed)
which(mlrdata$cooks.distance > 1)
# 5: optimal = 0
# (data points should be removed if cooks distance is close to 1)
which(mlrdata$leverage >= (3*mean(mlrdata$leverage)))
# 6: checking autocorrelation:
# Durbin-Watson test (optimal: high p-value)
dwt(m2.mlr)
# 7: test multicollinearity 1
vif(m2.mlr)
# 8: test multicollinearity 2
1/vif(m2.mlr)
# 9: mean vif should not exceed 1
mean(vif(m2.mlr))
# load functions
source("https://slcladal.github.io/rscripts/SampleSizeMLR.r")
source("https://slcladal.github.io/rscripts/ExpR.r")
# check if sample size is sufficient
smplesz(m2.mlr)
# check beta-error likelihood
expR(m2.mlr)
# load functions
source("./functions/SampleSizeMLR.r")
source("./functions/ExpR.r")
# check if sample size is sufficient
smplesz(m2.mlr)
# tabulate model results
sjPlot::tab_model(m0.glm, m2.glm)
sjPlot::tab_model(m2.glm)
summary(m2.mlr)
report::report(m2.mlr)
round(plogis(-10:10), 5)
plogis(-10:10) %>%
as.data.frame() %>%
dplyr::mutate(id = 1:n()) %>%
dplyr::rename(values = 1) %>%
ggplot(aes(y = values, x = id)) +
geom_line(color = "gray50", size = 1) +
geom_point(color = "red", size = 2)+
labs(x = "", y = "") +
theme(panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank())
# set seed
set.seed(12345)
# generates 20 values, with mean of 30 & s.d.=2
bodyheight=rnorm(20,175,20)
# sorts these values in ascending order
bodyheight=sort(bodyheight)
# assign 'survival' to these 20 individuals non-randomly
# most mortality occurs at smaller body size
relationship=c(0,0,0,0,0,1,0,1,0,0,1,1,0,1,1,1,0,1,1,1)
# saves data frame with two columns: body size, survival
blrdata=as.data.frame(cbind(bodyheight,relationship))
# generate models
lm1 <- lm(relationship ~ bodyheight, data = blrdata)
blr1 <- glm(relationship ~ bodyheight, family = binomial, data = blrdata)
blrdata <- blrdata %>%
dplyr::mutate(Pred_lm = predict(lm1, blrdata),
Pred_blm = predict(blr1, blrdata)) %>%
dplyr::mutate(Prob_lm = plogis(predict(lm1, blrdata)),
Prob_blm = plogis(predict(blr1, blrdata)),
Pred_blm2 = ifelse(predict(blr1, blrdata, type = "response") >= .5, 1, 0))
# plot
p1 <- ggplot(blrdata, aes(x = bodyheight, y =  relationship)) +
geom_point() +
geom_abline(intercept = coef(lm1)[1], slope = coef(lm1)[2], color = "red", size = .5) +
geom_point(aes(x = bodyheight, y = Pred_lm), color = "blue") +
theme_bw(base_size = 8)+
coord_cartesian(ylim = c(-0.2, 1.2), xlim = c(125, 225)) +
scale_y_continuous(breaks=seq(0, 1, 1), labels = c("Single", "Relationship")) +
guides(fill = FALSE) +
labs(title = "Predictions and \nregression line of lm.", x = "Height", y = "")
p2 <- ggplot(blrdata, aes(x = bodyheight, y =  relationship)) +
geom_point() +
geom_abline(intercept = coef(blr1)[1], slope = coef(blr1)[2], color = "red", size = .5) +
geom_point(aes(x = bodyheight, y = Pred_blm), color = "blue") +
theme_bw(base_size = 8)+
coord_cartesian(ylim = c(-0.2, 1.2), xlim = c(125, 225)) +
scale_y_continuous(breaks=seq(0, 1, 1), labels = c("Single", "Relationship")) +
guides(fill = FALSE) +
labs(title = "Predictions and \nregression line of blm.", x = "Height", y = "")
p3 <- ggplot(blrdata, aes(x = bodyheight, y =  relationship)) +
geom_point() +
geom_point(aes(x = bodyheight, y = Prob_blm), color = "blue") +
geom_smooth(method = "glm", method.args = list(family = "binomial"),
se = TRUE, color = "red", size = .5) +
geom_segment(aes(xend = bodyheight, yend = Prob_blm), color = "red", alpha = .2) +
theme_bw(base_size = 8) +
coord_cartesian(ylim = c(-0.2, 1.2), xlim = c(125, 225)) +
scale_y_continuous(breaks=seq(0, 1, 1), labels = c("Single", "Relationship")) +
guides(fill = FALSE) +
labs(title = "Logged Probabilities and \nlogged regression line of blm.", x = "Height", y = "")
# show plot
ggpubr::ggarrange(p1, p2, p3, nrow = 1)
# load data
blrdata  <- base::readRDS("./Data/bld.rda")
# load data
blrdata  <- base::readRDS("./Data/bld.rda")
# inspect data
blrdata %>%
as.data.frame() %>%
head(15) %>%
flextable() %>%
flextable::set_table_properties(width = .75, layout = "autofit") %>%
flextable::theme_zebra() %>%
flextable::fontsize(size = 12) %>%
flextable::fontsize(size = 12, part = "header") %>%
flextable::align_text_col(align = "center") %>%
flextable::set_caption(caption = "First 15 rows of the blrdata.")  %>%
flextable::border_outer()
blrdata <- blrdata %>%
# factorize variables
dplyr::mutate(Age = factor(Age),
Gender = factor(Gender),
Ethnicity = factor(Ethnicity),
ID = factor(ID),
EH = factor(EH)) %>%
# relevel Age (Reference = Young) and Ethnicity (Reference= Pakeha))
dplyr::mutate(Age = relevel(Age, "Young"),
Ethnicity = relevel(Ethnicity, "Pakeha"))
blrdata %>%
dplyr::mutate(EH = ifelse(EH == "0", 0, 1)) %>%
ggplot(aes(Age, EH, color = Gender)) +
facet_wrap(~Ethnicity) +
stat_summary(fun = mean, geom = "point") +
stat_summary(fun.data = mean_cl_boot, geom = "errorbar", width = 0.2) +
theme_set(theme_bw(base_size = 10)) +
theme(legend.position = "top") +
labs(x = "", y = "Observed Probabilty of eh") +
scale_color_manual(values = c("gray20", "gray70"))
?stat_summary
?fun.data
?mean_cl_boot
# set contrasts
options(contrasts  =c("contr.treatment", "contr.poly"))
# extract distribution summaries for all potential variables
blrdata.dist <- datadist(blrdata)
# store distribution summaries for all potential variables
options(datadist = "blrdata.dist")
blrdata.dist
# baseline glm model
m0.blr = glm(EH ~ 1, family = binomial, data = blrdata)
?ftable
ftable(blrdata$Age, blrdata$EH)
# check incomplete information
ifelse(min(ftable(blrdata$Age, blrdata$EH)) == 0, "not possible", "possible")
# add age to the model
m1.blr = glm(EH ~ Age, family = binomial, data = blrdata)
?vif
vif(m1.blr)
# check multicollinearity (vifs should have values of 3 or lower for main effects)
ifelse(max(vif(m1.blr)) <= 3,  "vifs ok", "WARNING: high vifs!") # VIFs ok
# check if adding Age significantly improves model fit
anova(m1.blr, m0.blr, test = "Chi")
ifelse(min(ftable(blrdata$Gender, blrdata$EH)) == 0, "not possible", "possible")
ftable(blrdata$Ethnicity, blrdata$EH)
ifelse(min(ftable(blrdata$Ethnicity, blrdata$EH)) == 0, "not possible", "possible")
ifelse(min(ftable(blrdata$Gender, blrdata$EH)) == 0, "not possible", "possible")
m2.blr <- update(m1.blr, . ~ . +Gender)
ifelse(max(vif(m2.blr)) <= 3,  "vifs ok", "WARNING: high vifs!") # VIFs ok
vif(m2.blr)
anova(m2.blr, m1.blr, test = "Chi")
Anova(m2.blr, test = "LR")
?Anova
?Anova()
anova(m2.blr, test = "LR")
Anova(m2.blr, test = "LR")
anova(m2.blr, test = "LR")
Anova(m2.blr, m1.blr, test = "Chi")
anova(m2.blr, m1.blr, test = "Chi")
?anova
ifelse(min(ftable(blrdata$Ethnicity, blrdata$EH)) == 0, "not possible", "possible")
m3.blr <- update(m2.blr, . ~ . +Ethnicity)
ifelse(max(vif(m3.blr)) <= 3,  "vifs ok", "WARNING: high vifs!") # VIFs ok
anova(m3.blr, m2.blr, test = "Chi")
ifelse(min(ftable(blrdata$Ethnicity, blrdata$EH)) == 0, "not possible", "possible")
m3.blr <- update(m2.blr, . ~ . +Ethnicity)
ifelse(max(vif(m3.blr)) <= 3,  "vifs ok", "WARNING: high vifs!") # VIFs ok
anova(m3.blr, m2.blr, test = "Chi")
Anova(m4.blr, m2.blr, test = "Chi")
?Anova
Anova(m4.blr, m2.blr, test = "Chisq")
Anova(m4.blr, m2.blr, test.statistic = "Chisq")
ifelse(min(ftable(blrdata$Age, blrdata$Gender, blrdata$EH)) == 0, "not possible", "possible")
m4.blr <- update(m2.blr, . ~ . +Age*Gender)
ifelse(max(vif(m4.blr)) <= 3,  "vifs ok", "WARNING: high vifs!") # VIFs ok
anova(m4.blr, m2.blr, test = "Chi")
Anova(m4.blr, m2.blr, test.statistic = "Chisq")
Anova(m4.blr, m2.blr, test = "Chisq")
anova(m4.blr, m2.blr, test = "Chi")
ifelse(min(ftable(blrdata$Age, blrdata$Ethnicity, blrdata$EH)) == 0, "not possible", "possible")
m5.blr <- update(m2.blr, . ~ . +Age*Ethnicity)
ifelse(max(vif(m5.blr)) <= 3,  "vifs ok", "WARNING: high vifs!") # VIFs ok
anova(m5.blr, m2.blr, test = "Chi")
ifelse(min(ftable(blrdata$Gender, blrdata$Ethnicity, blrdata$EH)) == 0, "not possible", "possible")
m6.blr <- update(m2.blr, . ~ . +Gender*Ethnicity)
ifelse(max(vif(m6.blr)) <= 3,  "vifs ok", "WARNING: high vifs!") # VIFs ok
anova(m6.blr, m2.blr, test = "Chi")
ifelse(min(ftable(blrdata$Age, blrdata$Gender, blrdata$Ethnicity, blrdata$EH)) == 0, "not possible", "possible")
m7.blr <- update(m2.blr, . ~ . +Gender*Ethnicity)
ifelse(max(vif(m7.blr)) <= 3,  "vifs ok", "WARNING: high vifs!") # VIFs ok
anova(m7.blr, m2.blr, test = "Chi")
m2.lrm <- lrm(EH ~ Age+Gender, data = blrdata, x = T, y = T, linear.predictors = T)
m2.lrm
?lrm
anova(m2.lrm)
validate(m2.lrm, bw = T, B = 200)
# model validation (remove # to activate: output too long for website)
m7.lrm <- lrm(EH ~ (Age+Gender+Ethnicity)^3, data = blrdata, x = T, y = T, linear.predictors = T)
validate(m7.lrm, bw = T, B = 200)
?validate
pentrace(m2.lrm, seq(0, 0.8, by = 0.05)) # determine penalty
lr.glm <- m2.blr  # rename final minimal adequate glm model
lr.lrm <- m2.lrm  # rename final minimal adequate lrm model
View(lr.lrm)
View(lr.glm)
modelChi <- lr.glm$null.deviance - lr.glm$deviance
chidf <- lr.glm$df.null - lr.glm$df.residual
ØŸpchisq
?pchisq
chisq.prob <- 1 - pchisq(modelChi, chidf)
modelChi
chidf
chisq.prob
modelChi; chidf; chisq.prob
anova(m0.glm, lr.glm, test = "Chi") # Model Likelihood Ratio Test
# calculate pseudo R^2
# number of cases
ncases <- length(fitted(lr.glm))
R2.hl <- modelChi/lr.glm$null.deviance
R.cs <- 1 - exp ((lr.glm$deviance - lr.glm$null.deviance)/ncases)
R.n <- R.cs /( 1- ( exp (-(lr.glm$null.deviance/ ncases))))
# function for extracting pseudo-R^2
logisticPseudoR2s <- function(LogModel) {
dev <- LogModel$deviance
nullDev <- LogModel$null.deviance
modelN <-  length(LogModel$fitted.values)
R.l <-  1 -  dev / nullDev
R.cs <- 1- exp ( -(nullDev - dev) / modelN)
R.n <- R.cs / ( 1 - ( exp (-(nullDev / modelN))))
cat("Pseudo R^2 for logistic regression\n")
cat("Hosmer and Lemeshow R^2  ", round(R.l, 3), "\n")
cat("Cox and Snell R^2        ", round(R.cs, 3), "\n")
cat("Nagelkerke R^2           ", round(R.n, 3),    "\n") }
logisticPseudoR2s(lr.glm)
# extract the confidence intervals for the coefficients
confint(lr.glm)
exp(lr.glm$coefficients) # odds ratios
exp(confint(lr.glm))     # confidence intervals of the odds ratios
# create variable with contains the prediction of the model
blrdata <- blrdata %>%
dplyr::mutate(Prediction = predict(lr.glm, type = "response"),
Prediction = ifelse(Prediction > .5, 1, 0),
Prediction = factor(Prediction, levels = c("0", "1")),
EH = factor(EH, levels = c("0", "1")))
# create a confusion matrix with compares observed against predicted values
caret::confusionMatrix(blrdata$Prediction, blrdata$EH)
# predicted probability
efp1 <- plot_model(lr.glm, type = "pred", terms = c("Age"), axis.lim = c(0, 1))
# predicted percentage
efp2 <- plot_model(lr.glm, type = "pred", terms = c("Gender"), axis.lim = c(0, 1))
grid.arrange(efp1, efp2, nrow = 1)
sjPlot::plot_model(lr.glm, type = "pred", terms = c("Age", "Gender"), axis.lim = c(0, 1)) +
theme(legend.position = "top") +
labs(x = "", y = "Predicted Probabilty of eh", title = "") +
scale_color_manual(values = c("gray20", "gray70"))
```{r prep2, warning=F, message=F}
# load packages
library(dplyr)
library(rms)
# create data set
# responses
# 100 random numbers
r1 <- rnorm(100, 50, 10)
# 50 smaller + 50 larger numbers
r2 <- c(r1[1:50], r1[51:100] + 50)
# predictors
a <- c(rep("1", 50), rep ("0", 50))
b <- rep(c(rep("1", 25), rep ("0", 25)), 2)
c <- rep(c(rep("1", 10), rep("0", 10)), 5)
d <- c(rep("1", 47), rep ("0", 3), rep ("0", 47), rep ("1", 3))
# create data set
df <- data.frame(r1, r2, a, b, c, d)
```
```{r prep2, warning=F, message=F}
# load packages
library(dplyr)
library(rms)
# create data set
# responses
# 100 random numbers
r1 <- rnorm(100, 50, 10)
# 50 smaller + 50 larger numbers
r2 <- c(r1[1:50], r1[51:100] + 50)
# predictors
a <- c(rep("1", 50), rep ("0", 50))
b <- rep(c(rep("1", 25), rep ("0", 25)), 2)
c <- rep(c(rep("1", 10), rep("0", 10)), 5)
d <- c(rep("1", 47), rep ("0", 3), rep ("0", 47), rep ("1", 3))
# create data set
df <- data.frame(r1, r2, a, b, c, d)
```
```{r data1, echo = F, message=FALSE, warning=FALSE}
# inspect data
df %>%
head(10) %>%
kable(caption = "First 10 rows of df data.") %>%
kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
full_width = F)
```
```{r prep2, warning=F, message=F}
# load packages
library(dplyr)
library(rms)
# create data set
# responses
# 100 random numbers
r1 <- rnorm(100, 50, 10)
# 50 smaller + 50 larger numbers
r2 <- c(r1[1:50], r1[51:100] + 50)
# predictors
a <- c(rep("1", 50), rep ("0", 50))
b <- rep(c(rep("1", 25), rep ("0", 25)), 2)
c <- rep(c(rep("1", 10), rep("0", 10)), 5)
d <- c(rep("1", 47), rep ("0", 3), rep ("0", 47), rep ("1", 3))
# create data set
df <- data.frame(r1, r2, a, b, c, d)
```
```{r data1, echo = F, message=FALSE, warning=FALSE}
# inspect data
df %>%
head(10) %>%
kable(caption = "First 10 rows of df data.") %>%
kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
full_width = F)
```
```{r prep2, warning=F, message=F}
# load packages
library(dplyr)
library(rms)
# create data set
# responses
# 100 random numbers
r1 <- rnorm(100, 50, 10)
# 50 smaller + 50 larger numbers
r2 <- c(r1[1:50], r1[51:100] + 50)
# predictors
a <- c(rep("1", 50), rep ("0", 50))
b <- rep(c(rep("1", 25), rep ("0", 25)), 2)
c <- rep(c(rep("1", 10), rep("0", 10)), 5)
d <- c(rep("1", 47), rep ("0", 3), rep ("0", 47), rep ("1", 3))
# create data set
df <- data.frame(r1, r2, a, b, c, d)
```
```{r prep2, warning=F, message=F}
# load packages
library(dplyr)
library(rms)
# create data set
# responses
# 100 random numbers
r1 <- rnorm(100, 50, 10)
# 50 smaller + 50 larger numbers
r2 <- c(r1[1:50], r1[51:100] + 50)
# predictors
a <- c(rep("1", 50), rep ("0", 50))
b <- rep(c(rep("1", 25), rep ("0", 25)), 2)
c <- rep(c(rep("1", 10), rep("0", 10)), 5)
d <- c(rep("1", 47), rep ("0", 3), rep ("0", 47), rep ("1", 3))
# create data set
df <- data.frame(r1, r2, a, b, c, d)
```
# load packages
library(dplyr)
library(rms)
# create data set
# responses
# 100 random numbers
r1 <- rnorm(100, 50, 10)
# 50 smaller + 50 larger numbers
r2 <- c(r1[1:50], r1[51:100] + 50)
# predictors
a <- c(rep("1", 50), rep ("0", 50))
b <- rep(c(rep("1", 25), rep ("0", 25)), 2)
c <- rep(c(rep("1", 10), rep("0", 10)), 5)
d <- c(rep("1", 47), rep ("0", 3), rep ("0", 47), rep ("1", 3))
# create data set
df <- data.frame(r1, r2, a, b, c, d)
# inspect data
df %>%
head(10) %>%
kable(caption = "First 10 rows of df data.") %>%
kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
full_width = F)
plot(r1, pch = 20)
plot(r2, pch = 20)
plot(r1, pch = 20)
plot(r2, pch = 20)
m1 <- lm(r1 ~ a + b + c + d, data = df)
# inspect model
summary(m1)
```{r}
# extract vifs
rms::vif(m1)
```
# extract vifs
rms::vif(m1)
# inspect data
df %>%
head(10) %>%
kable(caption = "First 10 rows of df data.") %>%
kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
full_width = F)
m2 <- lm(r2 ~ a + b + c + d, data = df)
# inspect model
summary(m2)
m2 <- lm(r2 ~d + a + b + c, data = df)
# inspect model
summary(m2)
m2 <- lm(r2 ~ a + b + c + d, data = df)
# inspect model
summary(m2)
# extract vifs
rms::vif(m2)
# inspect model matrix
mm <- model.matrix(m2)
# inspect data
mm %>%
head(15) %>%
kable(caption = "First 15 rows of the model matrix.") %>%
kableExtra::kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
full_width = F)
mt <- lm(mm[,5] ~ mm[,1:4])
summary(mt)$r.squared
R2.to.VIF <- function(some.modelmatrix.r2) {
return(1/(1-some.modelmatrix.r2)) }
R2.to.VIF(0.784)
